# TP3 - LLM con Razonamiento (FIUBA - PLN II)

## Consigna

> Implementar una aplicaciÃ³n que funcione como un **LLM con razonamiento**, el cual recibe una **pregunta compleja** y utiliza diferentes **agentes** para resolverla parcialmente, **compaginando todas las respuestas** para ofrecer una soluciÃ³n final.  
>
> AdemÃ¡s de la respuesta, se debe imprimir la cantidad de **tokens de entrada**, **tokens de salida** y **tokens de razonamiento** (estos Ãºltimos son los utilizados en las etapas intermedias).
>
> **Entregables:**  
> â€¢ Link al repositorio âœ…  
> â€¢ Video demostrativo âœ…

---

## ğŸ“š DescripciÃ³n del proyecto

Esta aplicaciÃ³n permite interactuar con un modelo de lenguaje grande (LLM) para:

- Resolver **preguntas complejas** descomponiÃ©ndolas en pasos mÃ¡s simples.
- **Razonar** paso a paso utilizando datos explÃ­citos entregados en el prompt.
- **Sumar los resultados parciales** para obtener una respuesta final.
- **Contabilizar** la cantidad de tokens de entrada, salida y razonamiento intermedio.

El proyecto cuenta con:
- **Una app de consola** (`main.py`) para consultas directas.
- **Una app web simple** (`app.py` + `index.html`) utilizando Flask.

---

## âš™ï¸ TecnologÃ­as utilizadas

- Python 3.11
- OpenAI / Groq API (Modelos Llama3)
- Flask (para la versiÃ³n web)
- Tiktoken (para contar tokens)

---


---

## ğŸš€ Â¿CÃ³mo correrlo?

### 1. Configurar variables de entorno

Crear un archivo `.env` con el siguiente contenido:

```bash
GROQ_API_KEY=tu_api_key



### 2. Instalar dependencias

pip install -r requirements.txt


### 3. Ejecutar modo consola

python src/main.py


### 4. Ejecutar modo web (opcional)

python src/app.py

Abrir navegador en http://127.0.0.1:5050/

